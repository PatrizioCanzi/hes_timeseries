{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "import glob \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contenus \n",
    "Les données sont telechargées depuis ENTSOE Transparency Platform et representent respectivement la charge electrique suisse et la production par source \n",
    "- Lecture et nettoyage des données brutes\n",
    "- Points d'attentions sur le traitement du temps\n",
    "- Analyse exploratoire\n",
    "- Decomposition en Trend/Saisonnalité/Residus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_files = glob.glob( '../data/load/*.csv')\n",
    "\n",
    "load_in = pd.concat((pd.read_csv(file) for file in load_files), ignore_index=True)\n",
    "\n",
    "print(load_in.shape)\n",
    "load_in.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_in.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_work = load_in.copy(deep = True)\n",
    "## Change column names \n",
    "load_work.columns = ['time','load_forecast','load']\n",
    "## Parse correctly the timestamp \n",
    "\n",
    "def parse_ts( df, column_name ):\n",
    "\n",
    "    ## Parses the timestamp from this format  e.g., '01.01.2019 00:00 - 01.01.2019 01:00'\n",
    "\n",
    "    df[['start','end']] = df[column_name].str.split('-' , expand = True )\n",
    "    df['dt'] = pd.to_datetime( df['start'], dayfirst=True, errors = 'coerce')\n",
    "\n",
    "    df.drop( columns = ['start','end',column_name ], inplace = True)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "## Parsing timestamps \n",
    "print( load_work.shape )\n",
    "load_work_parsed = parse_ts( load_work , 'time')\n",
    "print( load_work_parsed.shape) \n",
    "\n",
    "## Parsing numeric \n",
    "\n",
    "load_work_parsed['load'] = pd.to_numeric( load_work_parsed['load'] , errors = 'coerce' ) \n",
    "load_work_parsed['load_forecast'] = pd.to_numeric( load_work_parsed['load_forecast'] , errors = 'coerce' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_work_parsed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention to the null values !! \n",
    "load_work_parsed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir les nulls\n",
    "sns.heatmap( load_work_parsed.isnull()  )\n",
    "plt.title( 'Null Values for Load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Strategie: utiliser un mois complet jusqu'en octobre \n",
    "load_clean = load_work_parsed[load_work_parsed.dt < '2024-10-01 00:00:00']\n",
    "\n",
    "load_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Traitement du temps \n",
    "\n",
    "## Attention au referentiel de temps : CET \n",
    "## Controle des doublons\n",
    "## Controle des valeurs manquants \n",
    "\n",
    "load_clean.groupby( 'dt' ).count().sort_values( by = 'load' , ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention doit etre mise ! \n",
    "\n",
    "load_df = load_clean.copy( deep = True)\n",
    "print( load_df.shape ) \n",
    "load_df.drop_duplicates( subset = 'dt',  inplace = True , keep = 'first')\n",
    "print( load_df.shape ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df[load_df.load.isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examiner un cas particulier \n",
    "\n",
    "load_df[load_df.dt>'2023-03-26 00:00:00'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controler les nombre de timestamps \n",
    "load_df.groupby( load_df.dt.dt.year).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simplement ignorer les valeurs nulles\n",
    "# Il faudrait etre tres attentifs dans des cas de production à l'impact de tout cela\n",
    "#  \n",
    "load_df = load_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df.groupby( load_df.dt.dt.year).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df[load_df.dt>'2023-03-26 00:00:00'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Persist for later use \n",
    "load_df.to_csv( '..\\\\data\\\\curated_data\\\\load_clean.csv', index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention \n",
    "Jamais sousestimer cette partie car elle peut donner pas mal de problemes si elle n'est pas faite correctement ! \n",
    "\n",
    "Connaitre la matiere premiere est primordiale pour effectuer des bonnes analyses \n",
    "\n",
    "Ressources d'interet\n",
    "- https://fr.wikipedia.org/wiki/Temps_universel_coordonn%C3%A9 \n",
    "- https://fr.wikipedia.org/wiki/Heure_normale_d%27Europe_centrale \n",
    "- https://www.forecastclub.blog/2024/02/how-to-handle-time-series-missing-data.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality and exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line( data_frame = load_df , x = 'dt' , y = 'load' , title = 'Demande electrique en Suisse [MW]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.lineplot( data = load_df[:8000] , x = 'dt' , y = 'load' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_timeseries_with_granularity(df_in, ts_column, column, granularity, hue=None):\n",
    "    \"\"\"\n",
    "    Plot a Plotly line plot with granularity on the x-axis and the average of a column on the y-axis.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - ts_column: The column containing the timestamp or datetime.\n",
    "    - column: The column for which to calculate the average on the y-axis.\n",
    "    - granularity: The granularity for the x-axis (e.g., 'week', 'dayofweek', 'month', 'hour').\n",
    "    - hue: Optional; a column to differentiate lines in the plot (e.g., 'year', 'quarter').\n",
    "    \"\"\"\n",
    "    # Ensure ts_column is a datetime object\n",
    "\n",
    "    df = df_in.copy( deep = True )\n",
    "    df[ts_column] = pd.to_datetime(df[ts_column])\n",
    "\n",
    "    # Extract the desired granularity\n",
    "    if granularity == 'week':\n",
    "        df['granularity'] = df[ts_column].dt.isocalendar().week\n",
    "    elif granularity == 'dayofweek':\n",
    "        df['granularity'] = df[ts_column].dt.dayofweek\n",
    "    elif granularity == 'month':\n",
    "        df['granularity'] = df[ts_column].dt.month\n",
    "    elif granularity == 'hour':\n",
    "        df['granularity'] = df[ts_column].dt.hour\n",
    "    else:\n",
    "        raise ValueError(\"Invalid granularity. Choose from 'week', 'dayofweek', 'month', 'hour'.\")\n",
    "\n",
    "    # Optionally add 'year', 'quarter', or other hue options if provided\n",
    "    if hue == 'year':\n",
    "        df['hue'] = df[ts_column].dt.year\n",
    "    elif hue == 'quarter':\n",
    "        df['hue'] = df[ts_column].dt.quarter\n",
    "\n",
    "    # Group by granularity and hue (if provided) and calculate the mean of the column\n",
    "    if hue:\n",
    "        grouped = df.groupby(['granularity', 'hue'])[column].mean().reset_index()\n",
    "    else:\n",
    "        grouped = df.groupby(['granularity'])[column].mean().reset_index()\n",
    "\n",
    "    # Plot the graph using Plotly\n",
    "    if hue:\n",
    "        fig = px.line(grouped, x='granularity', y=column, color='hue',\n",
    "                      labels={'granularity': granularity.capitalize(), column: f'Average {column}', 'hue': hue.capitalize()},\n",
    "                      title=f'Average {column} by {granularity}')\n",
    "    else:\n",
    "        fig = px.line(grouped, x='granularity', y=column,\n",
    "                      labels={'granularity': granularity.capitalize(), column: f'Average {column}'},\n",
    "                      title=f'Average {column} by {granularity}')\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Analysis  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('tab10')\n",
    "plot_timeseries_with_granularity( load_df, ts_column = 'dt', column = 'load' , granularity= 'week' , hue = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('tab10')\n",
    "## Attention au dataset, combien de semaines en 2024?\n",
    " \n",
    "plot_timeseries_with_granularity( load_df, \n",
    "                                  ts_column = 'dt', \n",
    "                                  column = 'load' ,\n",
    "                                  granularity= 'dayofweek' , \n",
    "                                  hue = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('tab10')\n",
    "plot_timeseries_with_granularity( load_df, \n",
    "                                  ts_column = 'dt', \n",
    "                                  column = 'load' , \n",
    "                                  granularity= 'hour' , \n",
    "                                  hue = 'year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend, Seasonality, Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_seasonal_decompose(df, ts_column, column, model='additive', freq=None):\n",
    "    \"\"\"\n",
    "    Decomposes a time series into trend, seasonal, and residual components, and plots them using Matplotlib.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - ts_column: The column containing the timestamp or datetime.\n",
    "    - column: The column containing the time series values.\n",
    "    - model: The type of decomposition ('additive' or 'multiplicative').\n",
    "    - freq: Frequency of the time series (if not set, inferred automatically).\n",
    "    \"\"\"\n",
    "    # Ensure ts_column is a datetime object and set it as the index\n",
    "    df[ts_column] = pd.to_datetime(df[ts_column])\n",
    "    df = df.set_index(ts_column)\n",
    "\n",
    "    # Perform seasonal decomposition\n",
    "    decomposition = seasonal_decompose(df[column], model=model, period=freq, extrapolate_trend=True)\n",
    "\n",
    "    # Plot the decomposition components (observed, trend, seasonal, residual)\n",
    "    decomposition.plot()\n",
    "    plt.show()\n",
    "\n",
    "    return decomposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_load_df = load_df.set_index( 'dt').resample('MS').sum().reset_index()\n",
    "monthly_load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed  = simple_seasonal_decompose( monthly_load_df , ts_column= 'dt', column = 'load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La comprehension des residues peut aider à voir si la decomposition simple ne capture pas l'entierté du comportement, ou des effets \n",
    "#decomposed.resid.plot()\n",
    "#decomposed.resid.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time series differentes peuvent avoir comportement differents !  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation \n",
    "L'analyse des time serie temporelle peut donner des resultats differents selon la nature du phenomene observé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparation deja faite auparavant\n",
    "prd_in = pd.read_csv('..\\\\data\\\\curated_data\\\\generation_clean.csv')\n",
    "prd_in['dt'] = pd.to_datetime( prd_in['dt'])\n",
    " \n",
    "prd_df  = prd_in[['dt','solar','wind_onshore']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = go.Figure()\n",
    "for j in ['solar','wind_onshore']:\n",
    "    f.add_trace( go.Scatter( x = prd_df.dt , y = prd_df[j], name = j ))\n",
    "\n",
    "f.update_layout( title= 'Solar & Wind Generation [MW]')\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_df[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timeseries_with_granularity(prd_df , 'dt' , 'solar', 'month','year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timeseries_with_granularity(prd_df , 'dt' , 'solar', 'week','year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_solar_df = prd_df[['dt','solar']].set_index('dt').resample('MS').sum().reset_index()\n",
    "monthly_solar_df = monthly_solar_df[monthly_solar_df.dt.dt.year >= 2020]\n",
    "\n",
    "decomposed  = simple_seasonal_decompose( monthly_solar_df,\n",
    "                                         ts_column= 'dt', \n",
    "                                         column = 'solar' ,\n",
    "                                         model = 'multiplicative',\n",
    "                                         freq = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_solar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another Decomposition technique improves the shape of the decomposition. \n",
    "\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "stl = STL(monthly_solar_df.set_index('dt'), seasonal=13)  # Seasonal smoothing of 13 months (close to yearly cycle)\n",
    "result = stl.fit()\n",
    "\n",
    "# Plotting the components\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Original series\n",
    "ax1.plot(result.observed)\n",
    "ax1.set_ylabel('Observed')\n",
    "ax1.set_title('STL Decomposition of Solar Generation Time Series')\n",
    "\n",
    "# Trend component\n",
    "ax2.plot(result.trend)\n",
    "ax2.set_ylabel('Trend')\n",
    "\n",
    "# Seasonal component\n",
    "ax3.plot(result.seasonal)\n",
    "ax3.set_ylabel('Seasonal')\n",
    "\n",
    "# Residuals\n",
    "ax4.plot(result.resid)\n",
    "ax4.set_ylabel('Residuals')\n",
    "ax4.set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
